import numpy as np
from random import uniform
from random import randint
import random


from keras.datasets import mnist
import matplotlib.pyplot as plt

#MNIST

#load the MNIST dataset
(x_train, y_train), (_, _) = mnist.load_data()
print(x_train)
#Print 4 images in a row
plt.figure(figsize=(10, 5))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

#MNIST

x_train_flat = x_train.reshape(-1, 784)
x_train_flat = x_train_flat / 255.0

def one_hot(label, num_classes=10):
    vec = [0.0] * num_classes
    vec[label] = 1.0
    return vec

y_train_onehot = [one_hot(label) for label in y_train]


class Perceptron:
    def __init__(self, weights: list):
        self.weights = weights

    def sigmoid(self, x):
        return 1/(1+np.exp(-x))

    def weightedSum(self, inputs):
        return sum(x * w for x, w in zip(inputs, self.weights))

    def step(self, inputs):
        return self.sigmoid(self.weightedSum(inputs))
    
    def weightChange(self, newWeights: float, index: int):
        self.weights[index] = newWeights

    def training(self, inputs, expectedOutput: float, learning_rate=0.1, max_iters=1000):
        for _ in range(max_iters):

            output = self.step(inputs)

            error = expectedOutput - output

            if abs(error) < 1e-6:
                break
            
            for index in range(len(self.weights)):
                grad = error * output * (1 - output) * inputs[i]
                self.weights[index] += learning_rate * grad

class NeuralNetwork:
    def __init__(self, inputLayerL:int, hiddenLayersL:list, outputLayerL:int, initialWeightsRange:list):
        """
        description:
            Initializes a feedforward neural network with randomly assigned weights for each perceptron (neuron).
            The user specifies the number of neurons in the Input Layer, the structure of hidden layers, and the number of neurons in the output layer.

        inputLayerL: int
            Number of inputs (Input Neurons).

        hiddenLayersL: list of int
            List where each element specifies the number of perceptrons in a corresponding hidden layer.

        outputLayerL: int
            Number of Outputs (Output Neurons).
        """
        assert len(initialWeightsRange) == 2, 'incorrect ammount of arguments for initialWeightsRange'
        assert initialWeightsRange[0] <= 1.0 and initialWeightsRange[0] >= -1.0, 'incorrect inputfor the first value for initialWeightsRange'
        assert initialWeightsRange[1] <= 1.0 and initialWeightsRange[1] >= -1.0, 'incorrect input for second value for initialWeightsRange'

        self.inputLayer = inputLayerL
        self.hiddenLayers = []

        prevLayerSize = inputLayerL

        for layerSize in hiddenLayersL:
            layer = [Perceptron([uniform(initialWeightsRange[0], initialWeightsRange[1]) for _ in range(prevLayerSize)]) for _ in range(layerSize)]
            self.hiddenLayers.append(layer)
            prevLayerSize = layerSize

        self.outputLayer = [Perceptron([uniform(initialWeightsRange[0], initialWeightsRange[1]) for _ in range(prevLayerSize)]) for _ in range(outputLayerL)]

    def step(self, inputs: list):
        assert len(inputs) == self.inputLayer

        previousOutputs = inputs

        for layer in self.hiddenLayers:
            layerOutputs = []
            for perceptron in layer:
                layerOutputs.append(perceptron.step(previousOutputs))
            previousOutputs = layerOutputs

        outputs = []
        for perceptron in self.outputLayer:
            outputs.append(perceptron.step(previousOutputs))

        return outputs
    
    def forwardPass(self, inputs: list):
        assert len(inputs) == self.inputLayer

        activations = [inputs]
        weightedSums = []

        previousOutputs = inputs

        
        for layer in self.hiddenLayers:
            layerWeightedSums = []
            layerActivations = []
            
            for perceptron in layer:
                ws = perceptron.weightedSum(previousOutputs)
                act = perceptron.sigmoid(ws)
                layerWeightedSums.append(ws)
                layerActivations.append(act)

            weightedSums.append(layerWeightedSums)
            activations.append(layerActivations)
            previousOutputs = layerActivations


        outputWeightedSums = []
        outputActivations = []


        for perceptron in self.outputLayer:
           ws = perceptron.weightedSum(previousOutputs)
           act = perceptron.sigmoid(ws)
           outputWeightedSums.append(ws)
           outputActivations.append(act)


        weightedSums.append(outputWeightedSums)
        activations.append(outputActivations)


        return outputActivations, activations, weightedSums


    def lossCalculation(self, expectedValues: list, outputValues: list):
        return sum((yi - hat_yi) ** 2 for yi, hat_yi in zip(expectedValues, outputValues)) / len(expectedValues)

    def backPropagation(self, expectedValues: list, inputs: list, outputActivations: list, activations: list, weightedSums: list):

        deltaAllLayers = []

        output_deltas = []
        for j in range(len(self.outputLayer)):
            output = outputActivations[j]
            delta = (expectedValues[j] - output) * (output * (1 - output))
            output_deltas.append(delta)

        deltaAllLayers.append(output_deltas)

        next_layer = self.outputLayer
        next_deltas = output_deltas

        for layer_idx in reversed(range(len(self.hiddenLayers))):
            layer = self.hiddenLayers[layer_idx]
            layer_activations = activations[layer_idx + 1]
            layer_deltas = []

            for i, neuron_activation in enumerate(layer_activations):
                sum_delta = 0.0

                for j, perceptron in enumerate(next_layer):
                    weight = perceptron.weights[i]
                    sum_delta += weight * next_deltas[j]

                derivative = neuron_activation * (1 - neuron_activation)
                layer_deltas.append(sum_delta * derivative)

            deltaAllLayers.append(layer_deltas)
            next_layer = layer
            next_deltas = layer_deltas

        deltaAllLayers.reverse()
        return deltaAllLayers

    def WeightUpdates(self, activations: list, deltaAllLayers: list, learning_rate):
        for layer_idx, layer in enumerate(self.hiddenLayers):
            prev_activations = activations[layer_idx]
            deltas = deltaAllLayers[layer_idx]

            for perceptron_idx, perceptron in enumerate(layer):

                for w_idx in range(len(perceptron.weights)):
                    perceptron.weights[w_idx] += learning_rate * deltas[perceptron_idx] * prev_activations[w_idx]

        prev_activations = activations[-2]
        deltas = deltaAllLayers[-1]

        for perceptron_idx, perceptron in enumerate(self.outputLayer):

            for w_idx in range(len(perceptron.weights)):
                perceptron.weights[w_idx] += learning_rate * deltas[perceptron_idx] * prev_activations[w_idx]

    def Training(self, inputs, expectedOutput: list, learning_rate=0.1, max_iters=1000, print_every=50):
        for epoch in range(max_iters):
            outputActivations, activations, weightedSums = self.forwardPass(inputs)
            
            loss = self.lossCalculation(expectedOutput, outputActivations)
            
            if epoch % print_every == 0 or epoch == max_iters - 1 or loss < 1e-6:
                if max_iters > 1:
                    print(f"Epoch {epoch}, Loss: {loss:.4f}")
                else:
                    print (f"Loss: {loss:.4f}")

            deltaAllLayers = self.backPropagation(expectedOutput, inputs, outputActivations, activations, weightedSums)
            
            self.WeightUpdates(activations, deltaAllLayers, learning_rate)
            
            if loss < 1e-6:
                break



visionNetwork = NeuralNetwork(784, [784, 400, 64], 10, [-0.5, 0.5])

training_pairs = list(zip(x_train_flat, y_train_onehot))

epochLength = 10 #test if it works, will be bigger in a serious training
sampleSize = 50
#trains 5 times:
for e in range(epochLength):
    random.shuffle(training_pairs)
    #calls training
    print(f" EPOCH {e}:")
    for image in range(sampleSize):
        print(f"image {image}:".rstrip())
        visionNetwork.Training(training_pairs[image][0], training_pairs[image][1], 0.05, 1, 1)
